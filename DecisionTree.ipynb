{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import log2 as log\n",
    "import math\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preprocessing data.\n",
    "\n",
    "Load the csv file into pandas dataframe\n",
    "'''\n",
    "\n",
    "def GetData(target_attribute):\n",
    "    #~/Documents/SMAI/Assignment1/SamplePlayData.csv\n",
    "    #~/Documents/SMAI/Assignment1/decision_Tree/train.csv\n",
    "    data = pd.read_csv('~/Documents/SMAI/Assignment1/decision_Tree/train.csv')\n",
    "    dataframe = pd.DataFrame(data)\n",
    "    \n",
    "    num_of_rows = math.floor(len(dataframe[target_attribute]) * 0.8)\n",
    "    #dataframe = dataframe.sample(frac=1).reset_index(drop=True)\n",
    "    df = dataframe.copy()\n",
    "    dataframe = dataframe.head(num_of_rows)  #Train\n",
    "    test_data = df.tail(len(df[target_attribute]) - num_of_rows) #Test\n",
    "    actual_y = test_data['left'].values\n",
    "    test_data = test_data.drop(target_attribute, 1)\n",
    "    #train_data.sort() # sorts data\n",
    "    #test_data.sort()\n",
    "   \n",
    "    columns = list(dataframe.columns)\n",
    "    columns.remove(target_attribute)\n",
    "    return dataframe,test_data,actual_y,columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MissClassFormula(prob):\n",
    "    return min(prob,1-prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MissClass(dataframe,target_attribute):\n",
    "    unique = list(dataframe[target_attribute].unique())\n",
    "    return (GiniFormula(len(dataframe[target_attribute][dataframe[target_attribute] == unique[0]])/len(dataframe[target_attribute])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Works like  Information Gain\n",
    "'''\n",
    "def MissClassAttr(dataframe,attribute,target_attribute,MR):\n",
    "    target_variables = list(dataframe[target_attribute].unique())\n",
    "    unique_values = dataframe[attribute].unique()\n",
    "    M=0\n",
    "    for attr in unique_values:\n",
    "        prob = (len(dataframe[attribute][dataframe[attribute] == attr][dataframe[target_attribute] == target_variables[0]]))/len(dataframe[attribute][dataframe[attribute]==attr])\n",
    "        M += (len(dataframe[attribute][dataframe[attribute] == attr])/len(dataframe[attribute]))*GiniFormula(prob)\n",
    "    return MR-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "used by GiniAttr and Gini functions\n",
    "'''\n",
    "def GiniFormula(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    G = 2 * prob * (1-prob)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini(dataframe,target_attribute):\n",
    "    unique = list(dataframe[target_attribute].unique())\n",
    "    return (GiniFormula(len(dataframe[target_attribute][dataframe[target_attribute] == unique[0]])/len(dataframe[target_attribute])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Works like  Information Gain\n",
    "'''\n",
    "def GiniAttr(dataframe,attribute,target_attribute,GI):\n",
    "    target_variables = list(dataframe[target_attribute].unique())\n",
    "    unique_values = dataframe[attribute].unique()\n",
    "    G=0\n",
    "    for attr in unique_values:\n",
    "        prob = (len(dataframe[attribute][dataframe[attribute] == attr][dataframe[target_attribute] == target_variables[0]]))/len(dataframe[attribute][dataframe[attribute]==attr])\n",
    "        G += (len(dataframe[attribute][dataframe[attribute] == attr])/len(dataframe[attribute]))*GiniFormula(prob)\n",
    "    return GI-G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function is common to Entropy and InformationGain \n",
    "'''\n",
    "def EntropyFormula(prob):\n",
    "    if prob == 0 or prob == 1:\n",
    "        return 0\n",
    "    E = prob * np.log2(prob) + (1-prob) * np.log2(1-prob)\n",
    "    E = -1 * E\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entropy(dataframe,column):\n",
    "    unique_values = dataframe[column].unique()\n",
    "    E = 0\n",
    "    col_len = len(dataframe[column])\n",
    "    prob = (dataframe[column].value_counts()[unique_values[0]])/col_len\n",
    "    E = EntropyFormula(prob)\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ETotal ->  measure of the attribute \n",
    "'''\n",
    "def InformationGain(dataframe,column,target_attribute,target_values,ET):\n",
    "    unique_values = dataframe[column].unique()\n",
    "    val = target_values[0]\n",
    "    E=0\n",
    "    Intr=0 #Intrinsic\n",
    "    ETotal=0\n",
    "    N = len(dataframe[target_attribute])\n",
    "    for value in unique_values:\n",
    "        NI = dataframe[column].value_counts()[value]\n",
    "        prob = (len(dataframe[column][dataframe[column] == value][dataframe[target_attribute] == val]))/NI\n",
    "        E=EntropyFormula(prob)\n",
    "        Intr+=E    \n",
    "        E=(NI/N)*E\n",
    "        ETotal+=E\n",
    "    return ET-ETotal,Intr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_IG(split1,attribute_length,split_size,impurity):\n",
    "    prob1 = split1/split_size\n",
    "    if impurity == 'entropy':\n",
    "        E = EntropyFormula(prob1)\n",
    "    elif impurity == 'gini':\n",
    "        E = GiniFormula(prob1)\n",
    "    elif impurity == 'missclass':\n",
    "        E = MissClassFormula(prob1)\n",
    "    I = split_size/attribute_length\n",
    "    WE = I * E #weighted entropy\n",
    "    return WE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(impurity,dataframe,attribute,target_attribute,target_values,ET):\n",
    "    \n",
    "    df_u = np.unique(dataframe[attribute]).shape[0]\n",
    "    #print(\"[printing 1 ]\",df_u)\n",
    "    if df_u <= 1:\n",
    "        return (0,-2) \n",
    "    #print(\"[discrte] dataframe-->attribute\",attribute,dataframe)    \n",
    "    dataframe = dataframe.sort_values(by=[attribute])\n",
    "    df_attr_arr = dataframe[attribute].values\n",
    "    #print(\"[discrte] dataframe-->attribute\",attribute,dataframe)\n",
    "    listofIG = []\n",
    "    attribute_length = len(dataframe[attribute])\n",
    "    start_index = 0\n",
    "    WE_head,WE_tail,gain,split,high = 0,0,-5,-1,-3\n",
    "    #print(\"[discretize] dataframe[target_attribute].value_counts()-->\",dataframe[target_attribute].value_counts())\n",
    "    V1 = len(dataframe[target_attribute][dataframe[target_attribute] == target_values[0]])\n",
    "    V2 = attribute_length - V1\n",
    "    split1_V1,split1_V2=0,0\n",
    "    split2_V1,split2_V2=V1,V2\n",
    "    #print(\"[discretize] dataframe[attribute]-->\",dataframe[target_attribute])\n",
    "    df_array = dataframe[target_attribute].values\n",
    "    #print(\"[discrete] ET = \",ET)\n",
    "    #print(\"[discrete] V1 = \",V1,\" V2 = \",V2)\n",
    "    while start_index < (attribute_length-1):\n",
    "        if df_array[start_index] == target_values[0]:\n",
    "            split1_V1+=1\n",
    "            split2_V1-=1\n",
    "        else:\n",
    "            split1_V2+=1\n",
    "            split2_V2-=1\n",
    "        if df_attr_arr[start_index] != df_attr_arr[start_index+1]:\n",
    "            #print(\"[discrete] attr -->\",df_attr_arr[start_index])\n",
    "            tail_value = attribute_length - (start_index + 1)\n",
    "            WE_head = discrete_IG(split1_V1,attribute_length,start_index+1,impurity)\n",
    "            WE_tail = discrete_IG(split2_V1,attribute_length,tail_value,impurity)\n",
    "            gain = ET - (WE_head+WE_tail)\n",
    "            #print(gain)\n",
    "            listofIG.append(gain)\n",
    "            if gain > high:\n",
    "                high = gain\n",
    "                split = df_attr_arr[start_index] \n",
    "            \n",
    "        start_index+=1\n",
    "            \n",
    "    return high,split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculates Gain Ratio and returns the best attribute\n",
    "DOUBT: What happens when Intrinsic value is 0?\n",
    "'''\n",
    "def WinnerAttr(impurity,dataframe,target_attribute,target_values,ET,attributes,catcolumns=None,numcolumns=None):\n",
    "    GainRatio = []\n",
    "    splitArray=[]\n",
    "    split = -1\n",
    "    flag = 0\n",
    "    #print(\"[Winner Entropy] attributes->\",attributes)\n",
    "    #print(\"[WinnnerEntropy]\")\n",
    "    for attribute in attributes:\n",
    "        if numcolumns != None and attribute in numcolumns:\n",
    "            #print(\"[WinnerEntropy] Visited numeric column -- >\",attribute)\n",
    "            IG,split = discretize(impurity,dataframe,attribute,target_attribute,target_values,ET)\n",
    "            #print(\"[WinnerEntropy] Visited numeric column -- >\",attribute,\" Split -->\",split,\" IG-->\",IG)\n",
    "        else:  \n",
    "            split = -1\n",
    "            #print(\"[WinnerEntropy] Visited categoric column -- >\",attribute)\n",
    "            if impurity == 'entropy':\n",
    "                IG,Intr = InformationGain(dataframe,attribute,target_attribute,target_values,ET)\n",
    "            elif impurity == 'gini':\n",
    "                IG = GiniAttr(dataframe,attribute,target_attribute,ET)\n",
    "            elif impurity == 'missclass':\n",
    "                IG = MissClassAttr(dataframe,attribute,target_attribute,ET)\n",
    "            #print(\"[WinnerEntropy] Visited categorical column -- >\",attribute,\" Split -->\",split,\" IG-->\",IG)\n",
    "            #print(\"[WinnerEntropy] ET-->\",ET)\n",
    "        \n",
    "        GainRatio.append(IG) #divide by Intr for intrinsic\n",
    "        splitArray.append(split)\n",
    "    if len(set(GainRatio)) == 1 and (numcolumns is not None):   #Remove this or change logic later because only cat doesn't work \n",
    "        #print(\"HI\")\n",
    "        flag = 1\n",
    "    return attributes[np.argmax(GainRatio)],splitArray[np.argmax(GainRatio)],flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildTree(impurity,dataframe,target_attribute,target_values,ET,columns,catcolumns=None,numcolumns=None,Tree=None):\n",
    "    #print(\"[BuildTree] df \",dataframe)\n",
    "    if len(columns) == 0:\n",
    "        prob=0\n",
    "        a = len(dataframe[target_attribute][dataframe[target_attribute] == target_values[0]])\n",
    "        b = len(dataframe[target_attribute][dataframe[target_attribute] == target_values[1]])\n",
    "        if a>=b:\n",
    "            return target_values[0]\n",
    "        else:\n",
    "            return target_values[1]\n",
    "            \n",
    "    #if len(dataframe[target_attribute]) == 0:\n",
    "        #return -1\n",
    "    if impurity == 'entropy':    \n",
    "        ET = Entropy(dataframe,target_attribute)\n",
    "    elif impurity == 'gini':\n",
    "        ET = Gini(dataframe,target_attribute)\n",
    "    elif impurity == 'missclass':\n",
    "        ET = MissClass(dataframe,target_attribute)\n",
    "    Node,split,flag = WinnerAttr(impurity,dataframe,target_attribute,target_values,ET,columns,catcolumns,numcolumns) \n",
    "    #if type(Node) == int and Node == -1:\n",
    "        #return np.sum(dataframe[target_attribute]) >= dataframe.shape[0]\n",
    "    \n",
    "    if flag== 1:\n",
    "        prob=0\n",
    "        a = len(dataframe[target_attribute][dataframe[target_attribute] == target_values[0]])\n",
    "        b = len(dataframe[target_attribute][dataframe[target_attribute] == target_values[1]])\n",
    "        if a>=b:\n",
    "            return target_values[0]\n",
    "        else:\n",
    "            return target_values[1] \n",
    "            \n",
    "            \n",
    "    #print(\"[BuildTree] Node -->\",Node,\" Split-->\",split)\n",
    "    #print(\"[BuildTree]\",dataframe)\n",
    "    #print(\"[BuildTree] Unique attr --> \",len(dataframe[Node].unique()))\n",
    "    if Tree == None:\n",
    "        Tree = {}\n",
    "        Tree[Node] = {}\n",
    "    if split == -1:    \n",
    "        unique_values = list(dataframe[Node].unique())\n",
    "    else:\n",
    "        unique_values = []\n",
    "        unique_values.append(split)\n",
    "        unique_values.append(split+1)\n",
    "    for value in unique_values:\n",
    "        newcol = columns.copy()\n",
    "        if len(dataframe[target_attribute].unique())== 1:\n",
    "            return (list(dataframe[target_attribute].unique())[0])\n",
    "        else:\n",
    "            Tree[Node][value] = {}\n",
    "            #newcol.remove(Node)\n",
    "            if split == -1:\n",
    "                newcol.remove(Node)\n",
    "                Tree[Node][value]=BuildTree(impurity,dataframe[dataframe[Node] == value], \\\n",
    "                    target_attribute,target_values,ET,newcol,catcolumns,numcolumns)\n",
    "            else:\n",
    "                #print(\"[BuildTree] Node -->\",Node,\" Split-->\",split)\n",
    "                if value == unique_values[0]:\n",
    "                    #print(\"[BuildTree] Value->\",value)\n",
    "                    #dataframe = dataframe[dataframe[Node] <= split]\n",
    "                    Tree[Node][value] = BuildTree(impurity,dataframe[dataframe[Node] <= split],target_attribute, \\\n",
    "                                target_values,ET,newcol,catcolumns,numcolumns)\n",
    "                    #if a!=-1:Tree[Node][value]= a\n",
    "                else:\n",
    "                    #print(\"[BuildTree] Value->\",value)\n",
    "                    #dataframe = dataframe[dataframe[Node] > split]\n",
    "                    Tree[Node][value] = BuildTree(impurity,dataframe[dataframe[Node] > split], \\\n",
    "                                target_attribute,target_values,ET,newcol,catcolumns,numcolumns)\n",
    "                    #if a!=-1:Tree[Node][value]= a                \n",
    "    return Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(TreeModel,test_data,row,numcolumns):\n",
    "    while isinstance(TreeModel,dict):\n",
    "        attribute = next(iter(TreeModel))\n",
    "        #print(\"attribute-->\",attribute)\n",
    "        value = test_data[attribute].values[row]\n",
    "        #print(\"value -->\",value)\n",
    "        try:\n",
    "            if attribute not in numcolumns:\n",
    "                #print(TreeModel[attribute])\n",
    "                TreeModel = TreeModel[attribute][value]\n",
    "            else:\n",
    "                if value <= next(iter(TreeModel[attribute])):\n",
    "                    TreeModel = TreeModel[attribute][next(iter(TreeModel[attribute]))]\n",
    "                else:\n",
    "                    TreeModel = TreeModel[attribute][next(iter(TreeModel[attribute])) + 1]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "    return TreeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictY(TreeModel,test_data,numcolumns):\n",
    "    predicted_y = []\n",
    "    size = len(test_data['salary'])\n",
    "    #print(size)\n",
    "    i = 0\n",
    "    while i<= size-1:\n",
    "        #print(\"[predicted_y] i -->\",i)\n",
    "        y = predict(TreeModel,test_data,i,numcolumns)\n",
    "        #print(y)\n",
    "        predicted_y.append(y)\n",
    "        i+=1\n",
    "    return predicted_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Measure(TreeModel,test_data,numcolumns):\n",
    "    '''\n",
    "    Predict using Tree Model\n",
    "    '''\n",
    "\n",
    "    predicted_y = predictY(TreeModel,test_data,numcolumns)\n",
    "    #print(len(predicted_y))\n",
    "    #print(len(actual_y))\n",
    "    TP,TN,FP,FN = 0,0,0,0\n",
    "    j=0\n",
    "    for i in actual_y:\n",
    "        if i == 0 and i == predicted_y[j]:\n",
    "            TN+=1\n",
    "        if i == 0 and i!=predicted_y[j]:\n",
    "            FP+=1\n",
    "        if i == 1 and i == predicted_y[j]:\n",
    "            TP+=1\n",
    "        if i == 1 and i!=predicted_y[j]:\n",
    "            FN+=1\n",
    "        j+=1    \n",
    "\n",
    "\n",
    "    Accuracy = ((TP+TN)/(TP+TN+FP+FN))\n",
    "    Precision = (TP/(TP+FP))\n",
    "    Recall = (TP/(TP+FN))\n",
    "    FScore = 2/((1/Precision)+(1/Recall))\n",
    "    \n",
    "    \n",
    "    return Accuracy,Precision,Recall,FScore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Global initializations\n",
    "target_attribute = 'left'\n",
    "dataframe,test_data,actual_y,columns = GetData(target_attribute)\n",
    "catcolumns = ['sales','salary','promotion_last_5years','Work_accident']\n",
    "numcolumns = ['satisfaction_level','last_evaluation','number_project','average_montly_hours','time_spend_company']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing tree...\n",
      "Tree Created\n",
      "=========================================\n",
      "Accuracy :  0.7580071174377224\n",
      "Precision:  1.0\n",
      "Recall   :  0.001834862385321101\n",
      "F-Score  :  0.003663003663003663\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Q1. ENTROPY (Only categorical)\n",
    "target_values -> Yes/No\n",
    "'''\n",
    "impurity = 'entropy'\n",
    "target_values = dataframe[target_attribute].unique()\n",
    "print(\"Preparing tree...\")\n",
    "TreeModel = BuildTree(impurity,dataframe,target_attribute,target_values,None,catcolumns,None,None,None)\n",
    "print(\"Tree Created\")\n",
    "\n",
    "print(\"=========================================\")\n",
    "Accuracy,Precision,Recall,FScore = Measure(TreeModel,test_data,numcolumns)\n",
    "print(\"Accuracy : \",Accuracy)\n",
    "print(\"Precision: \",Precision)\n",
    "print(\"Recall   : \",Recall)\n",
    "print(\"F-Score  : \",FScore)\n",
    "print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing tree...\n",
      "Tree Created...\n",
      "=========================================\n",
      "Accuracy :  0.7580071174377224\n",
      "Precision:  1.0\n",
      "Recall   :  0.001834862385321101\n",
      "F-Score  :  0.003663003663003663\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Q1. GINI (Only categorical)\n",
    "'''\n",
    "impurity = 'gini'\n",
    "GI = Gini(dataframe,target_attribute)\n",
    "\n",
    "print(\"Preparing tree...\")\n",
    "TreeModel = BuildTree(impurity,dataframe,target_attribute,target_values,None,catcolumns,None,None,None)\n",
    "print(\"Tree Created...\")\n",
    "\n",
    "print(\"=========================================\")\n",
    "Accuracy,Precision,Recall,FScore = Measure(TreeModel,test_data,numcolumns)\n",
    "print(\"Accuracy : \",Accuracy)\n",
    "print(\"Precision: \",Precision)\n",
    "print(\"Recall   : \",Recall)\n",
    "print(\"F-Score  : \",FScore)\n",
    "print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing tree...\n",
      "Tree Created...\n",
      "=========================================\n",
      "Accuracy :  0.7580071174377224\n",
      "Precision:  1.0\n",
      "Recall   :  0.001834862385321101\n",
      "F-Score  :  0.003663003663003663\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Q1. Misclassification Error (Only categorical)\n",
    "'''\n",
    "impurity = 'missclass'\n",
    "GI = Gini(dataframe,target_attribute)\n",
    "\n",
    "print(\"Preparing tree...\")\n",
    "TreeModel = BuildTree(impurity,dataframe,target_attribute,target_values,None,catcolumns,None,None,None)\n",
    "print(\"Tree Created...\")\n",
    "\n",
    "print(\"=========================================\")\n",
    "Accuracy,Precision,Recall,FScore = Measure(TreeModel,test_data,numcolumns)\n",
    "print(\"Accuracy : \",Accuracy)\n",
    "print(\"Precision: \",Precision)\n",
    "print(\"Recall   : \",Recall)\n",
    "print(\"F-Score  : \",FScore)\n",
    "print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Tree...\n",
      "Tree created\n",
      "=========================================\n",
      "Accuracy :  0.9777580071174378\n",
      "Precision:  0.9508196721311475\n",
      "Recall   :  0.9577981651376147\n",
      "F-Score  :  0.9542961608775138\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Q2. ENTROPY with Continuous and Categorical\n",
    "'''\n",
    "impurity = 'entropy'\n",
    "target_values = dataframe[target_attribute].unique()\n",
    "print(\"Preparing Tree...\")\n",
    "TreeModel = BuildTree(impurity,dataframe,target_attribute,target_values,None,columns,None,numcolumns,None)\n",
    "print(\"Tree created\")\n",
    "\n",
    "print(\"=========================================\")\n",
    "Accuracy,Precision,Recall,FScore = Measure(TreeModel,test_data,numcolumns)\n",
    "print(\"Accuracy : \",Accuracy)\n",
    "print(\"Precision: \",Precision)\n",
    "print(\"Recall   : \",Recall)\n",
    "print(\"F-Score  : \",FScore)\n",
    "print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Tree...\n",
      "Tree created\n",
      "=========================================\n",
      "Accuracy :  0.9768683274021353\n",
      "Precision:  0.9393939393939394\n",
      "Recall   :  0.9669724770642202\n",
      "F-Score  :  0.9529837251356239\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Q2. GINI with Continuous and Categorical\n",
    "'''\n",
    "impurity = 'gini'\n",
    "target_values = dataframe[target_attribute].unique()\n",
    "print(\"Preparing Tree...\")\n",
    "TreeModel = BuildTree(impurity,dataframe,target_attribute,target_values,None,columns,None,numcolumns,None)\n",
    "print(\"Tree created\")\n",
    "\n",
    "print(\"=========================================\")\n",
    "Accuracy,Precision,Recall,FScore = Measure(TreeModel,test_data,numcolumns)\n",
    "print(\"Accuracy : \",Accuracy)\n",
    "print(\"Precision: \",Precision)\n",
    "print(\"Recall   : \",Recall)\n",
    "print(\"F-Score  : \",FScore)\n",
    "print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Tree...\n",
      "Tree created\n",
      "=========================================\n",
      "Accuracy :  0.9737544483985765\n",
      "Precision:  0.9278169014084507\n",
      "Recall   :  0.9669724770642202\n",
      "F-Score  :  0.9469901168014376\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Q2. MISCLASSIFICATION with Continuous and Categorical\n",
    "'''\n",
    "impurity = 'missclass'\n",
    "target_values = dataframe[target_attribute].unique()\n",
    "print(\"Preparing Tree...\")\n",
    "TreeModel = BuildTree(impurity,dataframe,target_attribute,target_values,None,columns,None,numcolumns,None)\n",
    "print(\"Tree created\")\n",
    "\n",
    "print(\"=========================================\")\n",
    "Accuracy,Precision,Recall,FScore = Measure(TreeModel,test_data,numcolumns)\n",
    "print(\"Accuracy : \",Accuracy)\n",
    "print(\"Precision: \",Precision)\n",
    "print(\"Recall   : \",Recall)\n",
    "print(\"F-Score  : \",FScore)\n",
    "print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
