{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "Given an input layer and series of hidden layer (zero or more) and an output layer of an artifical neural network we can feed this network an image in the input layer. This further, with weights and biases of each connection between any two layers, is trained at every neuron in the next layer, with every neuron learning a specfic part of the image. Finally the ouput layer which on the basic understanding consists of neurons equivalent of all possible labels. The neuron that activates the most is our desired label.\n",
    "\n",
    "<b>Steps:</b>\n",
    "1. $z = w^Tx+b$ (w is the weight vector and b is the bias vector)\n",
    "2. $a$ = $F(z)$ (where F is a funtion of z which Sigmoid,ReLu or tanh etc. to give a good distribution of real numbers)\n",
    "3. Repeat steps 1 and 2 till the output layer.\n",
    "\n",
    "\n",
    "### Back Propagation\n",
    "\n",
    "After Forward Propagation, we have a label that is predocted. That predicted may not be right as out network is yet to learn the precise weights and biases that predicts the correct label most of the time. Thus Backpropagation comes to picture where we find the difference between the true labels and out predicted labels. We then propagate these changes through each layer backwards. This is important as each layer's activaions is dependent on it's previous layers. This is continued for every image in our input. After one iteration over the images we have a set of errors at every layer.\n",
    "\n",
    "<b>Steps:</b>\n",
    "1. Finding delta: $\\delta_L$ = $\\dfrac{\\partial C}{\\partial a} * activation'(z) (Final layer)$\n",
    "2. Finding delta in hidden layers: $\\delta_L$ = $w^T_{L+1} * \\nabla\\delta_{L+1} * activation'(z)$\n",
    "3. Finding $\\nabla w_L$: $\\nabla w$ = $a_{L-1} * \\delta_L$\n",
    "4. Finding $\\nabla b_L$: $\\nabla b_L = \\delta_L$\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "We now make use of the errors colleceted at every layer aand use gradient descent i.e change weights and bias at every layer so as to correct the weights and biases as much as possible.\n",
    "\n",
    "The steps Forward Propagation and BackPropagation is done for every image in one iteration. Gradient descent is applied after every iteration of images. This is conitueed till we get errors approximating to zero.\n",
    "\n",
    "The above method can be slow as datasets can be huge in size. The bove process is called <b>Stochastic Gradient Descent</b>. One way to go around this problem is use <b>Mini-batch gradient descent</b> where we apply the gradient descent after a set of images have been fed to the network. This may result in slow convergence but doesn't affect the overall accuracy much.\n",
    "\n",
    "<b>Steps:</b>\n",
    "1. $w = w - \\alpha\\nabla w$\n",
    "2. $b = b - \\alpha\\nabla b$\n",
    "\n",
    "### Activation Functions:\n",
    "1. <b>Sigmoid(z): </b>$\\dfrac{1}{1+\\exp^{-z}}$<br><br>\n",
    "2. <b>Tanh(z): </b>$\\dfrac{2}{1+\\exp^{-2z}}-1$<br><br>\n",
    "3. <b>ReLU(z): </b><br>$z<=0, f(z) = 0$<br><br>$z>0, f(z)=z$<br><br>\n",
    "where $z = w^Tx+b$ (w is the weight vector and b is the bias vector) \n",
    "\n",
    "### Activation Function derivatives\n",
    "1. <b>Sigmoid'(z): </b>$Sigmoid(z) * (1-Sigmoid(z))$<br><br>\n",
    "2. <b>Tanh'(z): </b>$1- Tanh(z)^2$<br><br>\n",
    "3. <b>ReLU'(z): </b><br>$z<=0, f(z) = 0$<br><br>$z>0, f(z)=1$<br><br>\n",
    "where $z = w^Tx+b$ (w is the weight vector and b is the bias vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self,neurons,commonActivation,finalActivation):\n",
    "        self.neurons = neurons    #Number of Neurons8 in each layer\n",
    "        self.layers = len(neurons)  #Number of layers\n",
    "        self.biases = [np.random.randn(i,1) for i in neurons[1:]]  #Init Bias\n",
    "        self.weights = [np.random.randn(i,j) for i,j in zip(neurons[1:],neurons[:-1])]  #Init Weight\n",
    "        self.commonActivation = commonActivation\n",
    "        self.finalActivation = finalActivation\n",
    "     \n",
    "    def softmax(self,ycap):\n",
    "        #ycap = 10*batchsize\n",
    "        #ycapT = batchsize*10 \n",
    "        \n",
    "        ycap = ycap.transpose()\n",
    "        \n",
    "        '''\n",
    "        maxnum = np.max(ycap,axis=1)\n",
    "        maxnum = maxnum.reshape(maxnum.shape[0],1)\n",
    "        num = np.exp(ycap - maxnum)\n",
    "        denom = np.sum(np.exp(ycap),axis=1)\n",
    "        denom = np.array(denom).reshape((denom.shape[0],1))\n",
    "        return (num/denom).transpose()\n",
    "        '''\n",
    "        for row in range(0,ycap.shape[0]):\n",
    "            ycap[row] = ycap[row]-max(ycap[row])\n",
    "            ycap[row] = np.exp(ycap[row])/sum(np.exp(ycap[row]))\n",
    "        return(ycap.transpose())\n",
    "    \n",
    "    def activationFunc(self,X,activation):\n",
    "        if activation == 'sigmoid':\n",
    "            return 1/(1+np.exp(-X))\n",
    "        if activation == 'tanh':\n",
    "            return ((2/(1+np.exp(-2*X)))-1)\n",
    "        if activation == 'relu':\n",
    "            return X*(X>0)/700\n",
    "    \n",
    "    def activation_prime(self,z):\n",
    "        if self.finalActivation == 'sigmoid':\n",
    "            return (self.activationFunc(z,'sigmoid')*(1-self.activationFunc(z,'sigmoid')))\n",
    "        if self.finalActivation == 'tanh':\n",
    "            return (1-pow(self.activationFunc(z,'tanh'),2))\n",
    "        if self.finalActivation == 'relu':\n",
    "            return (1*(z>0))\n",
    "        \n",
    "    def crossEntropy(self,y,ycap):\n",
    "        entropy = -(y * np.log(ycap + 1.01)+ (1-y) * np.log(1 - ycap + 1.01))\n",
    "        #print(entropy) \n",
    "        return(entropy)\n",
    "    \n",
    "    def crossEntropyDeriv(self,y,ycap):\n",
    "        #for i in ycap:    \n",
    "        #deriv_cost = -1 * (y*(1/ycap)+(y-1)*(1/(1-ycap)))\n",
    "        deriv_cost = ycap - y\n",
    "        return(deriv_cost)\n",
    "    \n",
    "    def backprop(self,ycap,y,zs,activations):\n",
    "        n_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        n_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        y_actual = []\n",
    "        for i in y:\n",
    "            actual = np.zeros(10)\n",
    "            actual[i] = 1\n",
    "            y_actual.append(actual)\n",
    "        y = np.array(y_actual).T\n",
    "        #print(y.shape,ycap.shape)\n",
    "        delta_L = np.multiply(self.crossEntropyDeriv(y,self.softmax(ycap)),\\\n",
    "                                      self.activation_prime(zs[-1]))\n",
    "        \n",
    "        n_b[-1] = np.sum(delta_L,axis=1)\n",
    "        n_b[-1] = n_b[-1].reshape((n_b[-1].shape[0],1))\n",
    "        \n",
    "        n_w[-1] = np.dot(delta_L, activations[-2].transpose())\n",
    "        \n",
    "        for l in range(2, self.layers):\n",
    "            z = zs[-l]\n",
    "            delta_L = np.multiply(np.dot(self.weights[-l+1].transpose(), delta_L),\\\n",
    "                                              self.activation_prime(z))\n",
    "            \n",
    "            n_b[-l] = np.sum(delta_L,axis=1)\n",
    "            n_b[-l] = n_b[-l].reshape((n_b[-l].shape[0],1))\n",
    "            #print(\"bias \",n_b[-l].shape)\n",
    "            \n",
    "            n_w[-l] = np.dot(delta_L, activations[-l-1].transpose())\n",
    "            #print(delta_L.shape,activations[-l-1].transpose().shape)\n",
    "        return (n_b, n_w)\n",
    "        #return(delta)\n",
    "\n",
    "    \n",
    "    def feedforward(self,a):\n",
    "        activations = []\n",
    "        zs = []\n",
    "        activations.append(a) ##Added New\n",
    "        for weight,bias in zip(self.weights,self.biases):\n",
    "            #print((np.dot(weight,a)+bias).shape)\n",
    "            z = np.dot(weight,a)+bias\n",
    "            zs.append(z)\n",
    "            a = self.activationFunc(z,self.commonActivation)\n",
    "            activations.append(a)\n",
    "            #print(a.shape)\n",
    "        return(activations[-1],zs,activations)\n",
    "    \n",
    "    def fit(self,a,minibatch,epochs):\n",
    "        y_all = a[:,0]\n",
    "        a = a[:,1:]/255\n",
    "        k=0\n",
    "        eta=0.1\n",
    "        size = a.shape[0]\n",
    "        for epoch in range(0,epochs):\n",
    "            if epoch%10 == 0:\n",
    "                print(\"Epoch \",epoch)\n",
    "            k=0\n",
    "            while k<size:\n",
    "                #print(\"k+minibatch = \",k+minibatch)\n",
    "                batch = a[k:k+minibatch,:]\n",
    "                y = y_all[k:k+minibatch]\n",
    "                #print(\"Y length = \",len(y))\n",
    "                \n",
    "                ycap,zs,activations = self.feedforward(batch.T)\n",
    "                \n",
    "                #ycap = self.softmax(ycap)\n",
    "                \n",
    "                n_b,n_w = self.backprop(ycap,y,zs,activations)\n",
    "                \n",
    "                #Gradient Descent\n",
    "                self.weights = [w-(eta/minibatch)*nw for w, nw in zip(self.weights, n_w)]\n",
    "                self.biases = [b-(eta/minibatch)*nb for b, nb in zip(self.biases, n_b)]\n",
    "                k = k+minibatch\n",
    "            #print(\"Error: \",self.crossEntropy(y,ycap))\n",
    "            #print(ycap.shape)\n",
    "    \n",
    "    def predict(self,testdata):\n",
    "        y_all = testdata[:,0]\n",
    "        testdata = testdata[:,1:]/255\n",
    "        ycap,zs,activations = self.feedforward(testdata.T)\n",
    "        #print(ycap)\n",
    "        ycap = ycap.T\n",
    "        \n",
    "        y_new = []\n",
    "        for i in ycap:\n",
    "            y_new.append(np.argmax(i))\n",
    "        return (self.findTruevalues(y_all,y_new))\n",
    "   \n",
    "    def findTruevalues(self,y_all,y_new):\n",
    "        TP=0\n",
    "        size = len(y_all)\n",
    "        for x,y in zip(y_all,y_new):\n",
    "            if x == y:\n",
    "                TP+=1\n",
    "        return (TP/size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Apparel/apparel-trainval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Epoch  10\n",
      "Epoch  20\n",
      "Epoch  30\n",
      "Epoch  40\n",
      "0.7444166666666666\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "params :- \n",
    "NN(Neurons_in_each_layer,activation_in_all_layers,final_activation)\n",
    "model.fit(data,mini_batch_size,epoch)\n",
    "'''\n",
    "tempdata = np.array(data)[:48000,:]\n",
    "testdata = np.array(data)[48000:,:]\n",
    "neuralnets = NN([784,80,10],'sigmoid','sigmoid')\n",
    "neuralnets.fit(tempdata,100,50)\n",
    "accuracy = neuralnets.predict(testdata)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Epoch  1\n",
      "Epoch  2\n",
      "Epoch  3\n",
      "Epoch  4\n",
      "Epoch  5\n",
      "Epoch  6\n",
      "Epoch  7\n",
      "Epoch  8\n",
      "Epoch  9\n",
      "Epoch  10\n",
      "Epoch  11\n",
      "Epoch  12\n",
      "Epoch  13\n",
      "Epoch  14\n",
      "Epoch  15\n",
      "Epoch  16\n",
      "Epoch  17\n",
      "Epoch  18\n",
      "Epoch  19\n",
      "Epoch  20\n",
      "Epoch  21\n",
      "Epoch  22\n",
      "Epoch  23\n",
      "Epoch  24\n",
      "Epoch  25\n",
      "Epoch  26\n",
      "Epoch  27\n",
      "Epoch  28\n",
      "Epoch  29\n",
      "Epoch  30\n",
      "Epoch  31\n",
      "Epoch  32\n",
      "Epoch  33\n",
      "Epoch  34\n",
      "Epoch  35\n",
      "Epoch  36\n",
      "Epoch  37\n",
      "Epoch  38\n",
      "Epoch  39\n",
      "Epoch  40\n",
      "Epoch  41\n",
      "Epoch  42\n",
      "Epoch  43\n",
      "Epoch  44\n",
      "Epoch  45\n",
      "Epoch  46\n",
      "Epoch  47\n",
      "Epoch  48\n",
      "Epoch  49\n",
      "Epoch  50\n",
      "Epoch  51\n",
      "Epoch  52\n",
      "Epoch  53\n",
      "Epoch  54\n",
      "Epoch  55\n",
      "Epoch  56\n",
      "Epoch  57\n",
      "Epoch  58\n",
      "Epoch  59\n",
      "Epoch  60\n",
      "Epoch  61\n",
      "Epoch  62\n",
      "Epoch  63\n",
      "Epoch  64\n",
      "Epoch  65\n",
      "Epoch  66\n",
      "Epoch  67\n",
      "Epoch  68\n",
      "Epoch  69\n",
      "Epoch  70\n",
      "Epoch  71\n",
      "Epoch  72\n",
      "Epoch  73\n",
      "Epoch  74\n",
      "Epoch  75\n",
      "Epoch  76\n",
      "Epoch  77\n",
      "Epoch  78\n",
      "Epoch  79\n",
      "Epoch  80\n",
      "Epoch  81\n",
      "Epoch  82\n",
      "Epoch  83\n",
      "Epoch  84\n",
      "Epoch  85\n",
      "Epoch  86\n",
      "Epoch  87\n",
      "Epoch  88\n",
      "Epoch  89\n",
      "Epoch  90\n",
      "Epoch  91\n",
      "Epoch  92\n",
      "Epoch  93\n",
      "Epoch  94\n",
      "Epoch  95\n",
      "Epoch  96\n",
      "Epoch  97\n",
      "Epoch  98\n",
      "Epoch  99\n",
      "0.7454166666666666\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "params :- \n",
    "NN(Neurons_in_each_layer,activation_in_all_layers,final_activation)\n",
    "model.fit(data,mini_batch_size,epoch)\n",
    "'''\n",
    "tempdata = np.array(data)[:48000,:]\n",
    "testdata = np.array(data)[48000:,:]\n",
    "neuralnets = NN([784,64,64,10],'tanh','tanh')\n",
    "neuralnets.fit(tempdata,100,100)\n",
    "accuracy = neuralnets.predict(testdata)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Epoch  1\n",
      "Epoch  2\n",
      "Epoch  3\n",
      "Epoch  4\n",
      "Epoch  5\n",
      "Epoch  6\n",
      "Epoch  7\n",
      "Epoch  8\n",
      "Epoch  9\n",
      "Epoch  10\n",
      "Epoch  11\n",
      "Epoch  12\n",
      "Epoch  13\n",
      "Epoch  14\n",
      "Epoch  15\n",
      "Epoch  16\n",
      "Epoch  17\n",
      "Epoch  18\n",
      "Epoch  19\n",
      "Epoch  20\n",
      "Epoch  21\n",
      "Epoch  22\n",
      "Epoch  23\n",
      "Epoch  24\n",
      "Epoch  25\n",
      "Epoch  26\n",
      "Epoch  27\n",
      "Epoch  28\n",
      "Epoch  29\n",
      "Epoch  30\n",
      "Epoch  31\n",
      "Epoch  32\n",
      "Epoch  33\n",
      "Epoch  34\n",
      "Epoch  35\n",
      "Epoch  36\n",
      "Epoch  37\n",
      "Epoch  38\n",
      "Epoch  39\n",
      "Epoch  40\n",
      "Epoch  41\n",
      "Epoch  42\n",
      "Epoch  43\n",
      "Epoch  44\n",
      "Epoch  45\n",
      "Epoch  46\n",
      "Epoch  47\n",
      "Epoch  48\n",
      "Epoch  49\n",
      "Epoch  50\n",
      "Epoch  51\n",
      "Epoch  52\n",
      "Epoch  53\n",
      "Epoch  54\n",
      "Epoch  55\n",
      "Epoch  56\n",
      "Epoch  57\n",
      "Epoch  58\n",
      "Epoch  59\n",
      "Epoch  60\n",
      "Epoch  61\n",
      "Epoch  62\n",
      "Epoch  63\n",
      "Epoch  64\n",
      "Epoch  65\n",
      "Epoch  66\n",
      "Epoch  67\n",
      "Epoch  68\n",
      "Epoch  69\n",
      "Epoch  70\n",
      "Epoch  71\n",
      "Epoch  72\n",
      "Epoch  73\n",
      "Epoch  74\n",
      "Epoch  75\n",
      "Epoch  76\n",
      "Epoch  77\n",
      "Epoch  78\n",
      "Epoch  79\n",
      "Epoch  80\n",
      "Epoch  81\n",
      "Epoch  82\n",
      "Epoch  83\n",
      "Epoch  84\n",
      "Epoch  85\n",
      "Epoch  86\n",
      "Epoch  87\n",
      "Epoch  88\n",
      "Epoch  89\n",
      "Epoch  90\n",
      "Epoch  91\n",
      "Epoch  92\n",
      "Epoch  93\n",
      "Epoch  94\n",
      "Epoch  95\n",
      "Epoch  96\n",
      "Epoch  97\n",
      "Epoch  98\n",
      "Epoch  99\n",
      "0.86675\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "params :- \n",
    "NN(Neurons_in_each_layer,activation_in_all_layers,final_activation)\n",
    "model.fit(data,mini_batch_size,epoch)\n",
    "'''\n",
    "tempdata = np.array(data)[:48000,:]\n",
    "testdata = np.array(data)[48000:,:]\n",
    "neuralnets = NN([784,64,64,10],'relu','relu')\n",
    "neuralnets.fit(tempdata,100,100)\n",
    "accuracy = neuralnets.predict(testdata)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'reluweights'\n",
    "neuralnets.weights\n",
    "fileObject = open(filename,'wb')\n",
    "pickle.dump(neuralnets.weights,fileObject)\n",
    "fileObject.close()\n",
    "\n",
    "fileObject = open(filename,'rb')  \n",
    "# load the object from the file into var b\n",
    "b = pickle.load(fileObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Epoch  10\n",
      "Epoch  20\n",
      "Epoch  30\n",
      "Epoch  40\n",
      "Epoch  50\n",
      "Epoch  60\n",
      "Epoch  70\n",
      "Epoch  80\n",
      "Epoch  90\n",
      "0.567\n",
      "Epoch  0\n",
      "Epoch  10\n",
      "Epoch  20\n",
      "Epoch  30\n",
      "Epoch  40\n",
      "Epoch  50\n",
      "Epoch  60\n",
      "Epoch  70\n",
      "Epoch  80\n",
      "Epoch  90\n",
      "0.8525833333333334\n",
      "Epoch  0\n",
      "Epoch  10\n",
      "Epoch  20\n",
      "Epoch  30\n",
      "Epoch  40\n",
      "Epoch  50\n",
      "Epoch  60\n",
      "Epoch  70\n",
      "Epoch  80\n",
      "Epoch  90\n",
      "0.8705\n",
      "Epoch  0\n",
      "Epoch  10\n",
      "Epoch  20\n",
      "Epoch  30\n",
      "Epoch  40\n",
      "Epoch  50\n",
      "Epoch  60\n",
      "Epoch  70\n",
      "Epoch  80\n",
      "Epoch  90\n",
      "0.09725\n",
      "Epoch  0\n",
      "Epoch  10\n",
      "Epoch  20\n",
      "Epoch  30\n",
      "Epoch  40\n",
      "Epoch  50\n",
      "Epoch  60\n",
      "Epoch  70\n",
      "Epoch  80\n",
      "Epoch  90\n",
      "0.10058333333333333\n"
     ]
    }
   ],
   "source": [
    "#RELU\n",
    "'''\n",
    "params :- \n",
    "NN(Neurons_in_each_layer,activation_in_all_layers,final_activation)\n",
    "model.fit(data,mini_batch_size,epoch)\n",
    "'''\n",
    "neurons = 64\n",
    "layers = [784,10]\n",
    "accuracyList = []\n",
    "hiddenlist=[]\n",
    "while len(layers)<=6:\n",
    "    tempdata = np.array(data)[:48000,:]\n",
    "    testdata = np.array(data)[48000:,:]\n",
    "    neuralnets = NN(layers,'relu','relu')\n",
    "    neuralnets.fit(tempdata,100,100)\n",
    "    accuracy = neuralnets.predict(testdata)\n",
    "    accuracyList.append(accuracy)\n",
    "    hiddenlist.append(len(layers)-2)\n",
    "    layers.insert(-1,neurons)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Epoch  10\n",
      "Epoch  20\n",
      "Epoch  30\n",
      "Epoch  40\n",
      "Epoch  50\n",
      "Epoch  60\n",
      "Epoch  70\n",
      "Epoch  80\n",
      "Epoch  90\n",
      "0.6804166666666667\n",
      "Epoch  0\n",
      "Epoch  10\n",
      "Epoch  20\n",
      "Epoch  30\n",
      "Epoch  40\n",
      "Epoch  50\n",
      "Epoch  60\n",
      "Epoch  70\n",
      "Epoch  80\n",
      "Epoch  90\n",
      "0.48741666666666666\n",
      "Epoch  0\n",
      "Epoch  10\n",
      "Epoch  20\n",
      "Epoch  30\n",
      "Epoch  40\n",
      "Epoch  50\n",
      "Epoch  60\n",
      "Epoch  70\n",
      "Epoch  80\n",
      "Epoch  90\n",
      "0.1265\n",
      "Epoch  0\n",
      "Epoch  10\n",
      "Epoch  20\n",
      "Epoch  30\n",
      "Epoch  40\n",
      "Epoch  50\n",
      "Epoch  60\n",
      "Epoch  70\n",
      "Epoch  80\n",
      "Epoch  90\n",
      "0.7525833333333334\n",
      "Epoch  0\n",
      "Epoch  10\n",
      "Epoch  20\n",
      "Epoch  30\n",
      "Epoch  40\n",
      "Epoch  50\n",
      "Epoch  60\n",
      "Epoch  70\n",
      "Epoch  80\n",
      "Epoch  90\n",
      "0.7643333333333333\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "params :- \n",
    "NN(Neurons_in_each_layer,activation_in_all_layers,final_activation)\n",
    "model.fit(data,mini_batch_size,epoch)\n",
    "'''\n",
    "neurons = 64\n",
    "layers = [784,10]\n",
    "accuracyListSig = []\n",
    "hiddenlist=[]\n",
    "while len(layers)<=6:\n",
    "    tempdata = np.array(data)[:48000,:]\n",
    "    testdata = np.array(data)[48000:,:]\n",
    "    neuralnets = NN(layers,'sigmoid','sigmoid')\n",
    "    neuralnets.fit(tempdata,100,100)\n",
    "    accuracy = neuralnets.predict(testdata)\n",
    "    accuracyListSig.append(accuracy)\n",
    "    hiddenlist.append(len(layers)-2)\n",
    "    layers.insert(-1,neurons)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracyList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3cb4b3114934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiddenlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracyList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ReLU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiddenlistSig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracyList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ReLU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"No. of hidden Layers (64 each)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Accuracy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracyList' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(hiddenlist,accuracyList,label=\"ReLU\")\n",
    "plt.plot(hiddenlistSig,accuracyList,label=\"ReLU\")\n",
    "plt.xlabel = \"No. of hidden Layers (64 each)\"\n",
    "plt.ylabel = \"Accuracy\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
